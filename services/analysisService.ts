import { GoogleGenAI } from "@google/genai";
import { AppSettings, Message } from "../types";

interface ContextFile {
    name: string;
    content: string;
}

// Helper to format transcript
const formatTranscript = (messages: Message[], appName: string) => {
    return messages
        .filter(m => !m.isPartial)
        .map(m => {
            const time = new Date(m.timestamp).toLocaleTimeString();
            const speakerName = m.speaker ? m.speaker : (m.role === 'model' ? appName : 'User');
            return `[${time}] ${speakerName}: ${m.text}`;
        })
        .join('\n');
}

// Helper to format context files
const formatContextFiles = (files?: ContextFile[]) => {
    if (!files || files.length === 0) return "";
    return files.map((f, i) => `\n>>> è£œå……æ–‡ä»¶ ${i + 1}: ${f.name} <<<\n${f.content}\n-----------------------------------`).join('\n');
}

export const generateMeetingMinutes = async (
    messages: Message[],
    settings: AppSettings,
    meetingTitle: string,
    meetingDate: string,
    meetingDuration: string,
    customInstruction?: string,
    contextFiles?: ContextFile[]
): Promise<string> => {
    const transcript = formatTranscript(messages, settings.appName);
    const filesContent = formatContextFiles(contextFiles);

    const hasTranscript = transcript.trim().length > 0;
    const hasFiles = filesContent.trim().length > 0;

    // Only throw if NO transcript AND NO custom instruction AND NO files
    if (!hasTranscript && (!customInstruction || !customInstruction.trim()) && !hasFiles) {
        throw new Error("å°šç„¡é€å­—ç¨¿å…§å®¹æˆ–è£œå……è³‡æ–™ã€‚è«‹ç¢ºä¿éº¥å…‹é¢¨å·²é–‹å•Ÿä¸¦é–‹å§‹èªªè©±ï¼Œæˆ–ä¸Šå‚³æª”æ¡ˆï¼Œæˆ–è¼¸å…¥ç‰¹å®šæŒ‡ä»¤ã€‚");
    }

    const basePrompt = `
# è§’è‰²è¨­å®š
ä½ æ˜¯ä¸€ä½å°ˆç²¾æ–¼å•†æ¥­æœƒè­°èˆ‡å­¸è¡“è¨è«–çš„ã€${settings.appName} åŸ·è¡Œç§˜æ›¸ã€ã€‚ä½ çš„èªæ°£å°ˆæ¥­ã€å®¢è§€ä¸”çµæ§‹æ¢ç†åˆ†æ˜ã€‚

# ä»»å‹™
åˆ†ææä¾›çš„æœƒè­°é€å­—ç¨¿èˆ‡è£œå……è³‡æ–™ï¼Œä¸¦ç”Ÿæˆä¸€ä»½å®Œæ•´çš„ã€æœƒè­°å¾Œå ±å‘Šã€ã€‚
**æ‰€æœ‰å…§å®¹å¿…é ˆåš´æ ¼ä½¿ç”¨ã€ç¹é«”ä¸­æ–‡ (å°ç£)ã€‘æ’°å¯«ã€‚**

# è¼¸å‡ºæ ¼å¼ (è«‹åš´æ ¼éµå®ˆ Markdown æ ¼å¼)

# ${meetingTitle} - æœƒè­°è¨˜éŒ„
**æœƒè­°æ™‚é–“:** ${meetingDate}
**æœƒè­°æ™‚é•·:** ${meetingDuration}

## 1. ğŸ“„ æœƒè­°æ‘˜è¦ (Executive Summary)
> è«‹ç”¨ 3-5 å¥è©±æä¾›æœƒè­°çš„é«˜å±¤æ¬¡æ‘˜è¦ï¼ŒåŒ…å«ä¸»è¦ç›®çš„ã€é—œéµæ±ºè­°èˆ‡æ•´é«”çµè«–ã€‚ç›®æ¨™æ˜¯è®“ç®¡ç†è€…èƒ½åœ¨ 10 ç§’å…§æŒæ¡æœƒè­°é‡é»ã€‚

## 2. ğŸ”‘ é—œéµè­°é¡Œèˆ‡è¨è«– (Key Topics)
* **[è­°é¡Œ 1 åç¨±]**:
    * è¨è«–ç´°ç¯€èˆ‡è„ˆçµ¡...
    * æåˆ°çš„é—œéµæ•¸æ“šæˆ–è«–é»...
* **[è­°é¡Œ 2 åç¨±]**:
    * ç´°ç¯€...

## 3. âœ… æ±ºè­°äº‹é … (Decisions Made)
* [æ±ºè­° 1]: èªªæ˜æ±ºå®šäº†ä»€éº¼ã€‚
* [æ±ºè­° 2]: ...

## 4. ğŸš€ å¾…è¾¦äº‹é … (Action Items) - é‡è¦
*è«‹æå–å…·æœ‰æ˜ç¢ºè² è²¬äººçš„å¯åŸ·è¡Œä»»å‹™ï¼Œä¸¦ä½¿ç”¨è¡¨æ ¼æ ¼å¼å‘ˆç¾ã€‚*

| è² è²¬äºº (Owner) | å¾…è¾¦äº‹é … (Task) | æœŸé™/å„ªå…ˆç´š (Deadline) |
| :--- | :--- | :--- |
| @å§“å | ä»»å‹™æè¿°... | YYYY/MM/DD æˆ– é«˜/ä¸­/ä½ |
| @å§“å | ... | ... |

## 5. ğŸ’¡ å‚™è¨»èˆ‡ä¸‹æ¬¡æœƒè­° (Notes & Next Steps)
* **æœªæ±ºè­°é¡Œ:** åˆ—å‡ºå°šå¾…è§£æ±ºæˆ–éœ€è¦é€²ä¸€æ­¥è¨è«–çš„å•é¡Œã€‚
* **ä¸‹æ¬¡æœƒè­°:** æ—¥æœŸ/æ™‚é–“ æˆ– "å¾…å®š"ã€‚

# é™åˆ¶èˆ‡è¦ç¯„
- **èªè¨€:** åš´æ ¼ä½¿ç”¨ **ç¹é«”ä¸­æ–‡ (å°ç£)**ã€‚è«‹ä½¿ç”¨å°ç£æ…£ç”¨çš„å•†æ¥­è¡“èªï¼ˆä¾‹å¦‚ï¼šå°ˆæ¡ˆã€è¡ŒéŠ·ã€æ•¸æ“šã€å ±å‘Šï¼‰ã€‚
- **èªæ°£:** å°ˆæ¥­ã€ç²¾ç°¡ã€ä»¥è¡Œå‹•ç‚ºå°å‘ã€‚
- **æº–ç¢ºæ€§:** çµ•ä¸æé€ äº‹å¯¦ (No Hallucinations)ã€‚åªåŒ…å«é€å­—ç¨¿æˆ–è£œå……è³‡æ–™ä¸­å‡ºç¾çš„è³‡è¨Šã€‚
- **æ ¼å¼:** é‡å°é—œéµè¡“èªæˆ–é‡é»ä½¿ç”¨ **ç²—é«”** æ¨™ç¤ºã€‚
`;

    // Apply custom instruction logic
    let finalPrompt = basePrompt;

    if (customInstruction && customInstruction.trim()) {
        finalPrompt = `
# è§’è‰²è¨­å®š
ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ AI æœƒè­°åŠ©ç†ã€‚

# ä»»å‹™
è«‹æ ¹æ“šæä¾›çš„æœƒè­°é€å­—ç¨¿èˆ‡è£œå……è³‡æ–™ï¼Œä¸¦**åš´æ ¼éµå®ˆä½¿ç”¨è€…çš„ä»¥ä¸‹æŒ‡ä»¤**ä¾†ç”Ÿæˆæˆ–ä¿®æ”¹å…§å®¹ã€‚

---
### ğŸ”´ ä½¿ç”¨è€…ç‰¹åˆ¥æŒ‡ä»¤ (æœ€é«˜å„ªå…ˆç´š)ï¼š
"${customInstruction}"
---

å¦‚æœä½¿ç”¨è€…æŒ‡ä»¤è¦æ±‚ç‰¹å®šçš„æ ¼å¼ï¼ˆå¦‚è¡¨æ ¼ã€ç¿»è­¯ã€æ‘˜è¦é‡é»ï¼‰ï¼Œè«‹å„ªå…ˆæ»¿è¶³è©²è¦æ±‚ï¼Œå¿½ç•¥ä¸‹æ–¹çš„é è¨­æ ¼å¼ã€‚
å¦‚æœä½¿ç”¨è€…æŒ‡ä»¤è¼ƒç‚ºæ¨¡ç³Šï¼ˆå¦‚ã€Œæ•´ç†æœƒè­°è¨˜éŒ„ã€ï¼‰ï¼Œå‰‡åƒè€ƒä¸‹æ–¹çš„é è¨­æ ¼å¼ã€‚

æ³¨æ„ï¼šå¦‚æœé€å­—ç¨¿å…§å®¹ç‚ºç©ºä½†æœ‰è£œå……è³‡æ–™ï¼Œè«‹æ ¹æ“šè£œå……è³‡æ–™é€²è¡Œæ‘˜è¦æˆ–å›ç­”ã€‚

# é è¨­åƒè€ƒæ ¼å¼ (åƒ…åœ¨ä½¿ç”¨è€…ç„¡ç‰¹å®šæ ¼å¼è¦æ±‚æ™‚åƒè€ƒ)
${basePrompt}
      `;
    }

    const systemPrompt = `
${finalPrompt}

---

ã€è£œå……åƒè€ƒè³‡æ–™ (Context Files)ã€‘
${hasFiles ? filesContent : "(ç„¡)"}

---
    
ã€æœƒè­°é€å­—ç¨¿å…§å®¹ã€‘
${hasTranscript ? transcript : "(ç›®å‰å°šç„¡é€å­—ç¨¿å…§å®¹)"}
  `;

    return await callLLM(systemPrompt, settings);
};

export const chatWithTranscript = async (
    messages: Message[],
    userQuestion: string,
    history: { role: string, content: string }[],
    settings: AppSettings,
    contextFiles?: ContextFile[]
): Promise<string> => {
    const transcript = formatTranscript(messages, settings.appName);
    const filesContent = formatContextFiles(contextFiles);

    const conversationStr = history.map(h => `${h.role === 'user' ? 'User' : 'AI'}: ${h.content}`).join('\n');

    const systemPrompt = `
    ä½ æ˜¯ä¸€ä½è°æ˜çš„æœƒè­°åŠ©æ‰‹ (Chatbot)ã€‚
    ä½ çš„ä»»å‹™æ˜¯ï¼š**å›ç­”ä½¿ç”¨è€…é—œæ–¼ã€Œæœƒè­°é€å­—ç¨¿ã€èˆ‡ã€Œè£œå……è³‡æ–™ã€çš„ç‰¹å®šå•é¡Œ**ã€‚
    
    é‡è¦è¦å‰‡ï¼š
    1. **ä¸è¦**ä¸»å‹•ç”¢ç”Ÿæœƒè­°æ‘˜è¦ï¼Œé™¤éä½¿ç”¨è€…æ˜ç¢ºè¦æ±‚ã€‚
    2. **ä¸è¦**é‡è¤‡ä½¿ç”¨è€…çš„å•é¡Œã€‚
    3. ç›´æ¥é‡å°å•é¡Œå›ç­”ï¼Œç­”æ¡ˆç›¡é‡ç²¾ç°¡ã€‚
    4. å¿…é ˆæ ¹æ“šä¸‹æ–¹æä¾›çš„ã€æœƒè­°é€å­—ç¨¿ã€‘èˆ‡ã€è£œå……åƒè€ƒè³‡æ–™ã€‘å…§å®¹å›ç­”ã€‚å¦‚æœæ‰¾ä¸åˆ°ç›¸é—œè³‡è¨Šï¼Œè«‹èª å¯¦å›ç­”ã€Œè³‡æ–™ä¸­æœªæåŠæ­¤å…§å®¹ã€ã€‚
    5. è«‹ä½¿ç”¨èˆ‡æœƒè­°å…§å®¹ä¸€è‡´çš„èªè¨€ï¼ˆé€šå¸¸æ˜¯ç¹é«”ä¸­æ–‡ï¼‰å›ç­”ã€‚
    
    ---
    ã€è£œå……åƒè€ƒè³‡æ–™ (Context Files)ã€‘
    ${filesContent ? filesContent : "(ç„¡)"}
    ---

    ã€æœƒè­°é€å­—ç¨¿ã€‘
    ${transcript}
    ---
    
    ã€å…ˆä¾†çš„å°è©±ç´€éŒ„ã€‘
    ${conversationStr}
    
    ã€ä½¿ç”¨è€…çš„ç•¶å‰å•é¡Œã€‘
    ${userQuestion}
    `;

    return await callLLM(systemPrompt, settings);
};

const sleep = (ms: number) => new Promise(resolve => setTimeout(resolve, ms));

async function callLLM(prompt: string, settings: AppSettings, retries = 3): Promise<string> {
    try {
        if (settings.provider === 'gemini') {
            if (!settings.apiKeys.gemini) throw new Error("è«‹å…ˆåœ¨è¨­å®šä¸­è¼¸å…¥ Gemini API Keyã€‚");

            // Use hardcoded model if not set, BUT respect the user setting if present.
            // Note: The error message mentioned 'gemini-3-flash', so user might be using that.
            const modelName = settings.geminiAnalysisModel || 'gemini-2.0-flash-exp';

            const ai = new GoogleGenAI({ apiKey: settings.apiKeys.gemini });

            const response = await ai.models.generateContent({
                model: modelName,
                contents: {
                    role: 'user',
                    parts: [{ text: prompt }]
                }
            });
            return response.text || "Gemini æœªå›å‚³ä»»ä½•å…§å®¹ã€‚";
        }

        if (settings.provider === 'openai') {
            if (!settings.apiKeys.openai) throw new Error("è«‹å…ˆåœ¨è¨­å®šä¸­è¼¸å…¥ OpenAI API Keyã€‚");

            const response = await fetch("https://api.openai.com/v1/chat/completions", {
                method: "POST",
                headers: {
                    "Content-Type": "application/json",
                    "Authorization": `Bearer ${settings.apiKeys.openai}`
                },
                body: JSON.stringify({
                    model: "gpt-4o",
                    messages: [
                        { role: "system", content: `You are ${settings.appName}, a helpful meeting assistant.` },
                        { role: "user", content: prompt }
                    ]
                })
            });

            if (!response.ok) {
                const err = await response.json();
                throw new Error(`OpenAI Error: ${err.error?.message || response.statusText}`);
            }

            const data = await response.json();
            return data.choices?.[0]?.message?.content || "OpenAI æœªå›å‚³ä»»ä½•å…§å®¹ã€‚";
        }

        if (settings.provider === 'ollama') {
            const url = settings.ollamaUrl.replace(/\/$/, '');
            let response;
            try {
                response = await fetch(`${url}/api/chat`, {
                    method: "POST",
                    headers: { "Content-Type": "application/json" },
                    body: JSON.stringify({
                        model: "llama3",
                        messages: [
                            { role: "user", content: prompt }
                        ],
                        stream: false
                    })
                });
            } catch (e: any) {
                throw new Error(`ç„¡æ³•é€£ç·šè‡³ Ollama (${url})ã€‚\nè«‹æª¢æŸ¥ï¼š\n1. Ollama æ˜¯å¦å·²å•Ÿå‹•ï¼Ÿ\n2. è‹¥ç‚º HTTPS ç¶²é ï¼Œç„¡æ³•é€£ç·š HTTP æœ¬åœ°æœå‹™(æ··åˆå…§å®¹)ã€‚\n3. æ˜¯å¦é–‹å•Ÿ CORSï¼Ÿ(OLLAMA_ORIGINS="*")`);
            }

            if (!response.ok) throw new Error("Ollama é€£ç·šæˆåŠŸä½†å›å‚³éŒ¯èª¤ï¼Œè«‹æª¢æŸ¥æ¨¡å‹åç¨±æ˜¯å¦æ­£ç¢ºã€‚");

            const data = await response.json();
            return data.message?.content || "Ollama æœªå›å‚³ä»»ä½•å…§å®¹ã€‚";
        }

        if (settings.provider === 'lmstudio') {
            const url = settings.lmStudioUrl.replace(/\/$/, '');

            // Check if model is loaded (optional but good practice with LM Studio APIs)
            // Skipping detailed check for brevity unless errors persist.

            let response;
            try {
                response = await fetch(`${url}/chat/completions`, {
                    method: "POST",
                    headers: {
                        "Content-Type": "application/json",
                        "Authorization": "Bearer lm-studio"
                    },
                    body: JSON.stringify({
                        model: "local-model",
                        messages: [
                            { role: "system", content: `You are ${settings.appName}.` },
                            { role: "user", content: prompt }
                        ],
                        temperature: 0.7
                    })
                });
            } catch (e: any) {
                throw new Error(`ç„¡æ³•é€£ç·šè‡³ LM Studio (${url})ã€‚\nå¯èƒ½åŸå› ï¼š\n1. LM Studio æœªå•Ÿå‹• Serverã€‚\n2. è‹¥ç¶²é ç‚º HTTPS (å¦‚ Vercel)ï¼Œç€è¦½å™¨æœƒé˜»æ“‹é€£ç·šè‡³ HTTP (è«‹æ”¹ç”¨ Localhost é–‹ç™¼æˆ– ngrok)ã€‚\n3. è·¨åŸŸ (CORS) è¢«é˜»æ“‹ (è«‹åœ¨ LM Studio è¨­å®šé–‹å•Ÿ CORS)ã€‚\n4. è‹¥ä½¿ç”¨å€ç¶² IPï¼Œè«‹ç¢ºèª LM Studio å…è¨±å¤–éƒ¨é€£ç·šã€‚`);
            }

            if (!response.ok) throw new Error("å·²é€£ç·š LM Studio ä½†å›å‚³éŒ¯èª¤ï¼Œè«‹ç¢ºèªæ¨¡å‹æ˜¯å¦å·²è¼‰å…¥ (ç‹€æ…‹æ‡‰ç‚º Loaded)ã€‚");

            const data = await response.json();
            return data.choices?.[0]?.message?.content || "LM Studio æœªå›å‚³ä»»ä½•å…§å®¹ã€‚";
        }

        if (settings.provider === 'anythingllm') {
            const url = settings.anythingLlmUrl.replace(/\/$/, '');
            let response;
            try {
                response = await fetch(`${url}/chat/completions`, {
                    method: "POST",
                    headers: {
                        "Content-Type": "application/json",
                        "Authorization": `Bearer ${settings.apiKeys.anythingllm || 'dummy'}`
                    },
                    body: JSON.stringify({
                        model: "anythingllm",
                        messages: [
                            { role: "system", content: `You are ${settings.appName}.` },
                            { role: "user", content: prompt }
                        ]
                    })
                });
            } catch (e: any) {
                throw new Error(`ç„¡æ³•é€£ç·šè‡³ AnythingLLM (${url})ã€‚\nè«‹æª¢æŸ¥ Server ç‹€æ…‹ã€CORS è¨­å®šæˆ–æ··åˆå…§å®¹ (Mixed Content) å•é¡Œã€‚`);
            }

            if (!response.ok) {
                let errorMsg = response.statusText;
                try {
                    const err = await response.json();
                    errorMsg = err.message || errorMsg;
                } catch (e) { }
                throw new Error(`AnythingLLM å›å‚³éŒ¯èª¤: ${errorMsg}`);
            }

            const data = await response.json();
            return data.choices?.[0]?.message?.content || "AnythingLLM æœªå›å‚³ä»»ä½•å…§å®¹ã€‚";
        }

        throw new Error(`ä¾›æ‡‰å•† ${settings.provider} å°šæœªå¯¦ä½œã€‚`);

    } catch (error: any) {
        // Handle Rate Limiting (429) specifically
        if (retries > 0 && (error.message?.includes('429') || error.message?.includes('RESOURCE_EXHAUSTED') || error.status === 429)) {
            // Try to extract wait time from error message, e.g., "Please retry in 57.89569092s."
            let waitSeconds = 20;
            const match = error.message?.match(/retry in\s+([0-9.]+)\s*s/i);
            if (match && match[1]) {
                waitSeconds = Math.ceil(parseFloat(match[1])) + 2; // Add 2s buffer
            } else if (error.message?.includes('Quota exceeded')) {
                // Fallback for quota exceeded which might be longer
                waitSeconds = 60;
            }

            console.warn(`Rate limit hit. Retrying in ${waitSeconds}s... (${retries} retries left)`);

            // Wait
            await sleep(waitSeconds * 1000);
            return callLLM(prompt, settings, retries - 1);
        }

        console.error("LLM Error:", error);
        throw new Error(error.message || "AI è™•ç†å¤±æ•—ã€‚");
    }
}